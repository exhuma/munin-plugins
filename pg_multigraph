#!/usr/bin/env python

'''
Munin Postgres Plugins

TODO doc

Changes to Official Perl Plugins:

    * Locks only show both waiting and granted locks. The same lock has the same
      color hue. Granted locks are muc brighter than waiting locks.
'''

from __future__ import print_function
from collections import namedtuple
from os import getenv
from psycopg2 import connect  # TODO <- munin autoconfig
from psycopg2.extras import DictCursor
from textwrap import dedent
import argparse
import logging
import re

LOG = logging.getLogger(__name__)


INVALID_CHARS = re.compile(r'[^a-zA-Z0-9_]')
ConnectionCounter = namedtuple(
    'ConnectionCounter',
    'username idle idle_tx unknown query_running waiting')
Lock = namedtuple('Lock', 'mode, granted')
GraphedValue = namedtuple('GraphedValue', 'name, label, doc')
DbHealth = namedtuple('DbHealth', 'dbname vacuum_age analyze_age')


def find_subclasses(cls):
    output = set()
    for subcls in cls.__subclasses__():
        if hasattr(subcls, 'NAME'):
            output.add(subcls)
        output = output | find_subclasses(subcls)
    return output


def construct_dsn(dbname, user, password='', host='', port=0):
    elements = [
        'dbname=%s' % dbname,
        'user=%s' % user
    ]
    if password:
        elements.append('password=%s' % password)
    if host:
        elements.append('host=%s' % host)
    if port:
        elements.append('port=%d' % port)

    return ' '.join(elements)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('action', nargs='?',
                        help='The munin action to perform',
                        default='fetch')
    return parser.parse_args()


class PostgresPlugin(object):

    def __init__(self, connection, user, password, host, port):
        self.connection = connection
        self.user = user
        self.password = password
        self.host = host
        self.port = port
        self._dbnames = []

    def fetch(self):
        print('multigraph %s' % self.graph_name)

    def config(self):
        print('multigraph %s' % self.graph_name)
        print('graph_category postgresql_multi')
        print('graph_title %s' % self.TITLE)

    @property
    def graph_name(self):
        return 'pgmg_%s' % self.NAME

    @property
    def all_databases(self):
        if self._dbnames:
            return self._dbnames
        cursor = self.connection.cursor()
        cursor.execute('SELECT datname FROM pg_database '
                       'WHERE datistemplate=false')
        dbnames = [row[0] for row in cursor]
        cursor.close()
        return dbnames


class Connections(PostgresPlugin):

    NAME = 'connections'
    TITLE = 'Connections'

    def all_connections(self):
        # Fetch the current username of the connection
        cursor = self.connection.cursor()
        cursor.execute('SELECT CURRENT_USER')
        whoami = cursor.fetchone()[0]
        cursor.close()

        query = dedent(
            '''\
            WITH users AS (SELECT usename FROM pg_user),
            conntype AS (SELECT u.usename, act.waiting,
                current_query
                FROM users u
                LEFT JOIN pg_stat_activity act USING (usename))
            SELECT
                usename,
                COUNT(CASE WHEN current_query='<IDLE>'
                    THEN 1 END) AS idle,
                COUNT(CASE WHEN current_query='<IDLE> in transaction'
                    THEN 1 END) AS idle_tx,
                COUNT(CASE WHEN current_query='<insufficient privilege>'
                    THEN 1 END) AS unknown,
                COUNT(CASE WHEN current_query NOT IN (
                    '<IDLE>',
                    '<IDLE> in transaction',
                    '<insufficient privilege>')
                    THEN 1 END) AS query_running,
                COUNT(CASE WHEN waiting THEN 1 END) AS waiting
            FROM conntype
            GROUP BY usename
            ORDER BY usename;''')
        cursor = self.connection.cursor()
        cursor.execute(query)
        output = []
        for username, idle, idle_tx, unknown, query_running, waiting in cursor:
            if username == whoami:  # Subtract our own active query
                query_running = query_running - 1
            output.append(ConnectionCounter(username, idle, idle_tx, unknown,
                                            query_running, waiting))
        cursor.close()
        return output

    def fetch(self):
        super(Connections, self).fetch()
        conns = self.all_connections()
        sums = {
            'idle': sum([conn.idle for conn in conns]),
            'idle_transaction': sum([conn.idle_tx for conn in conns]),
            'unknown': sum([conn.unknown for conn in conns]),
            'query_running': sum([conn.query_running for conn in conns]),
            'waiting': sum([conn.waiting for conn in conns]),
        }

        print('idle.value %d' % sums['idle'])
        print('idle_transaction.value %d' % sums['idle_transaction'])
        print('unknown.value %d' % sums['unknown'])
        print('query_running.value %d' % sums['query_running'])
        print('waiting.value %d' % sums['waiting'])

        # print values for the each subgraph
        for subgraph in conns:
            print('multigraph %s.%s' % (
                self.graph_name,
                INVALID_CHARS.sub('_', subgraph.username)))
            print('idle.value %d' % subgraph.idle)
            print('idle_transaction.value %d' % subgraph.idle_tx)
            print('unknown.value %d' % subgraph.unknown)
            print('query_running.value %d' % subgraph.query_running)
            print('waiting.value %d' % subgraph.waiting)

    def config(self):
        super(Connections, self).config()
        # We want the main and subgraph to use the same config. So we store it
        # as a variable.
        common_config_block = dedent(
            '''\
            graph_args --base 1000
            graph_vlabel connections
            graph_order waiting query_running idle idle_transaction unknown
            graph_printf %3.0lf
            waiting.label Waiting for lock
            waiting.draw AREA
            query_running.label Active query
            query_running.draw STACK
            idle.label Idle connections
            idle.draw STACK
            idle_transaction.label Idle transaction
            idle_transaction.draw STACK
            unknown.label unknown
            unknown.draw STACK
            ''')

        conns = self.all_connections()

        print('graph_info Shows an overview of connections and their type on '
              'the PostgreSQL cluster.')

        print(common_config_block)
        for subgraph in conns:
            clean_username = INVALID_CHARS.sub('_', subgraph.username)
            print('multigraph %s.%s' % (self.graph_name, clean_username))
            print('graph_title %s for user %s' % (
                self.TITLE, subgraph.username))
            print(common_config_block)


class Sizes(PostgresPlugin):

    NAME = 'sizes'
    TITLE = 'Database Sizes'

    def get_stats(self):
        cursor = self.connection.cursor()
        cursor.execute('SELECT datname, pg_database_size(datname) '
                       'FROM pg_database WHERE datistemplate=false')
        sizes = cursor.fetchall()
        cursor.close()
        return {row[0]: row[1] for row in sizes}

    def fetch(self):
        super(Sizes, self).fetch()
        stats = self.get_stats()

        for dbname, value in stats.items():
            print('%s.value %s' % (INVALID_CHARS.sub('_', dbname), value))

        for dbname, value in stats.items():
            print('multigraph %s.%s' % (self.graph_name, dbname))
            print('size.value %s' % value)

    def config(self):
        super(Sizes, self).config()
        stats = self.get_stats()
        print(dedent(
            '''\
            graph_args --base 1024
            graph_vlabel Size in Bytes
            '''))

        first_graph = True
        for dbname in sorted(stats):
            print(dedent(
                '''\
                {clean_name}.info Size in Bytes for database {clean_name}
                {clean_name}.label {raw_name}
                {clean_name}.draw {style}
                {clean_name}.min 0
                '''.format(raw_name=dbname,
                           clean_name=INVALID_CHARS.sub('_', dbname),
                           style='AREA' if first_graph else 'STACK')))
            first_graph = False

        # Scale each graph relative to the biggest DB
        upper_limit = max(stats.values())
        template = dedent(
            '''\
            multigraph {graph_name}.{dbname}
            graph_title {title} for {dbname}
            graph_args --base 1024 --upper-limit {upper_limit}
            graph_order size
            size.min 0
            size.draw AREA
            size.label Size (in Bytes)
            ''')
        for dbname, value in stats.items():
            print(template.format(title=self.TITLE,
                                  graph_name=self.graph_name,
                                  dbname=dbname,
                                  upper_limit=upper_limit))


class Locks(PostgresPlugin):

    NAME = 'locks'
    TITLE = 'Locks'

    COLORS = [
        ('ff0000', 'ff8080'),
        ('ffbf00', 'ffdf80'),
        ('80ff00', 'bfff80'),
        ('00ff40', '80ff9f'),
        ('00ffff', '80ffff'),
        ('0040ff', '809fff'),
        ('7f00ff', 'bf80ff'),
        ('ff00bf', 'ff80df'),
    ]
    ACCEPTED_LOCK_NAMES = {
        GraphedValue('accesssharelock', 'Access share', ''),
        GraphedValue('rowsharelock', 'Row share', ''),
        GraphedValue('rowexclusivelock', 'Row excl.', ''),
        GraphedValue('shareupdateexclusivelock', 'Share upd. excl.', ''),
        GraphedValue('sharelock', 'Share', ''),
        GraphedValue('sharerowexclusivelock', 'Share row excl.', ''),
        GraphedValue('exclusivelock', 'Exclusive', ''),
        GraphedValue('accessexclusivelock', 'Access excl.', ''),
    }

    def _graph_config(self):
        first_graph = True
        for i, lock_info in enumerate(sorted(self.ACCEPTED_LOCK_NAMES)):
            waiting_color, granted_color = self.COLORS[i]
            print(dedent(
                '''\
                {lock.name}_granted.label {lock.label} (granted)
                {lock.name}_granted.draw {style}
                {lock.name}_granted.min 0
                {lock.name}_granted.colour {gcolour}
                {lock.name}_waiting.label {lock.label} (waiting)
                {lock.name}_waiting.draw STACK
                {lock.name}_waiting.min 0
                {lock.name}_waiting.colour {wcolour}
                '''.format(lock=lock_info,
                           style='AREA' if first_graph else 'STACK',
                           gcolour=granted_color,
                           wcolour=waiting_color)))
            first_graph = False

    def get_stats(self):
        query = dedent(
            '''\
            SELECT
                db.datname,
                LOWER(mode),
                locktype,
                granted,
                COUNT(mode)
            FROM pg_database db
            FULL OUTER JOIN pg_locks lck ON (db.oid=lck.database)
            GROUP BY db.datname, mode, locktype, granted;
            ''')
        cursor = self.connection.cursor()
        cursor.execute(query)
        locks = cursor.fetchall()
        cursor.close()
        output = {}
        for dbname, lockmode, locktype, granted, count in locks:
            if dbname is None:
                # locks without related DB, are "global" locks
                dbname = '__pg__database__'

            dblocks = output.setdefault(dbname, {})
            dblocks[Lock(lockmode, granted)] = count

        # The above query only returns rows for databases with actual
        # waiting/granted locks. We always want all DBs to be monitored though.
        # DBs which don't have locks should report the value `0`. This prevents
        # NaN values ("holes") in munin.
        for row in self.all_databases:
            output.setdefault(row, {})
        return output

    def fetch(self):
        super(Locks, self).fetch()
        stats = self.get_stats()

        sums = {}
        for row in stats.values():
            for lock_info in self.ACCEPTED_LOCK_NAMES:
                granted_lock = Lock(lock_info.name, True)
                waiting_lock = Lock(lock_info.name, False)
                granted_value = sums.setdefault(granted_lock, 0)
                granted_value += row.get(granted_lock, 0)
                waiting_value = sums.setdefault(waiting_lock, 0)
                waiting_value += row.get(waiting_lock, 0)
                sums[granted_lock] = granted_value
                sums[waiting_lock] = waiting_value

        for lock, value in sums.items():
            print('%s_%s.value %s' % (lock.mode,
                                      'granted' if lock.granted else 'waiting',
                                      value))

        for dbname, values in sorted(stats.items()):
            print('multigraph %s.%s' % (self.graph_name, dbname))
            for lock_info in self.ACCEPTED_LOCK_NAMES:
                granted_lock = Lock(lock_info.name, True)
                waiting_lock = Lock(lock_info.name, False)
                print('%s_granted.value %d' % (lock_info.name,
                                               values.get(granted_lock, 0)))
                print('%s_waiting.value %d' % (lock_info.name,
                                               values.get(waiting_lock, 0)))

    def config(self):
        super(Locks, self).config()
        stats = self.get_stats()
        print(dedent(
            '''\
            graph_args --base 1000
            graph_vlabel Number of locks
            graph_printf %3.0lf
            '''))

        self._graph_config()

        for dbname, value in stats.items():
            print('multigraph %s.%s' % (self.graph_name, dbname))
            print('graph_title %s for %s' % (self.TITLE, dbname))
            self._graph_config()


class QueryAges(PostgresPlugin):

    NAME = 'query_ages'
    TITLE = 'Query Ages (seconds)'

    def get_stats(self):
        query = dedent(
            '''\
            SELECT
                datname,
                MAX(extract(EPOCH FROM NOW() - query_start))
            FROM pg_stat_activity
            GROUP BY datname
            ''')
        cursor = self.connection.cursor()
        cursor.execute(query)
        oldest_queries = cursor.fetchall()
        cursor.close()
        output = {}
        for dbname, age in oldest_queries:
            output[dbname] = age

        # The above query only returns rows for databases with actual
        # waiting/granted locks. We always want all DBs to be monitored though.
        # DBs which don't have locks should report the value `0`. This prevents
        # NaN values ("holes") in munin.
        # TODO If we do this before the cursor loop, we can use a
        # dict-comprehension
        for row in self.all_databases:
            output.setdefault(row, 0)
        return output

    def fetch(self):
        super(QueryAges, self).fetch()
        stats = self.get_stats()

        max_age = 0
        for row in stats.values():
            max_age = max(max_age, row)
        print('age.value %f' % max_age)

        for dbname, value in sorted(stats.items()):
            print('multigraph %s.%s' % (self.graph_name, dbname))
            print('age.value %f' % value)

    def config(self):
        super(QueryAges, self).config()
        stats = self.get_stats()
        print(dedent(
            '''\
            graph_args --base 1000
            graph_vlabel Query Age (seconds)
            graph_printf %3.2lf
            age.label Age
            age.min 0
            '''))

        for dbname, value in stats.items():
            print('multigraph %s.%s' % (self.graph_name, dbname))
            print('graph_title %s for %s' % (self.TITLE, dbname))
            print('age.label Age')
            print('age.min 0')


class IOPlugin(PostgresPlugin):

    def _print_values(self, values):
        for field, _, _ in self.FIELDS:
            print('%s.value %s' % (field, values.get(field, 0) or 0))

    def _graph_config(self):
        # given the linear list, we need to combine them as pairs so we can
        # process the related items as one.
        field_pairs = zip(self.FIELDS[::2], self.FIELDS[1::2])

        first_graph = True
        for read, hit in field_pairs:
            read_name, read_label, read_info = read
            hit_name, hit_label, hit_info = hit
            stack_type = 'AREA' if first_graph else 'STACK'

            print('%s.label %s' % (read_name, read_label))
            print('%s.info %s' % (read_name, hit_info))
            print('%s.min 0' % read_name)
            print('%s.type DERIVE' % read_name)
            print('%s.graph no' % read_name)
            print('%s.draw %s' % (read_name, stack_type))

            print('%s.label %s' % (hit_name, hit_label))
            print('%s.info %s' % (hit_name, hit_info))
            print('%s.min 0' % hit_name)
            print('%s.type DERIVE' % hit_name)
            print('%s.negative %s' % (hit_name, read_name))
            print('%s.draw %s' % (hit_name, stack_type))
            first_graph = False

    def fetch(self):
        super(IOPlugin, self).fetch()
        stats = self.get_stats()

        sums = {}
        for count in stats.values():
            for field_name, _, _ in self.FIELDS:
                current_value = sums.setdefault(field_name, 0) or 0
                actual = count.get(field_name, 0) or 0
                sums[field_name] = current_value + actual
        self._print_values(sums)

        for dbname in self.all_databases:
            print('multigraph %s.%s' % (self.graph_name, dbname))
            self._print_values(stats.get(dbname, {}))

    def config(self):
        super(IOPlugin, self).config()
        print(dedent(
            '''\
            graph_args --base 1000
            graph_vlabel blocks unbuffered (-) / buffered (+)
            graph_printf %3.0lf
            '''))
        self._graph_config()

        for dbname in self.all_databases:
            print('multigraph %s.%s' % (self.graph_name, dbname))
            print('graph_title %s for %s' % (self.TITLE, dbname))
            self._graph_config()


class TableDiskIO(IOPlugin):

    NAME = 'tableio'
    TITLE = 'Disk/Buffer IOs (User Tables)'

    # Fields are processed as read/hit pairs. They need to follow this sequence
    # in this list!
    FIELDS = [
        ('heap_blks_read',
         'Heap blocks',
         ''),
        ('heap_blks_hit',
         'Heap blocks',
         'Number of disk blocks read (-) / buffered (+)'),
        ('idx_blks_read',
         'Index blocks',
         ''),
        ('idx_blks_hit',
         'Index blocks',
         'Number of disk blocks read (-) / buffered (+) from all indexes'),
        ('toast_blks_read',
         'TOAST blocks',
         ''),
        ('toast_blks_hit',
         'TOAST blocks',
         'Number of disk blocks read (-) / buffered (+) from TOAST tables '
         '(if any)'),
        ('tidx_blks_read',
         'TOAST index blocks',
         ''),
        ('tidx_blks_hit',
         'TOAST index blocks',
         'Number of disk blocks read (-) / buffered (+) from TOAST table '
         'indexes (if any)'),
    ]

    def get_stats(self):
        # we need to get stats for each DB. We need to create new connections
        # for each.
        dbnames = self.all_databases

        # Fetch stats for each DB
        stats_query = dedent(
            '''\
            SELECT
                SUM(heap_blks_read) AS heap_blks_read,
                SUM(heap_blks_hit) AS heap_blks_hit,
                SUM(idx_blks_read) AS idx_blks_read,
                SUM(idx_blks_hit) AS idx_blks_hit,
                SUM(toast_blks_read) AS toast_blks_read,
                SUM(toast_blks_hit) AS toast_blks_hit,
                SUM(tidx_blks_read) AS tidx_blks_read
            FROM pg_statio_user_tables;
            ''')

        stats = {}
        for dbname in dbnames:
            try:
                localcon = connect(
                    construct_dsn(dbname,
                                  self.user,
                                  self.password,
                                  self.host,
                                  self.port))
            except Exception as exc:
                LOG.error(exc)
                continue

            cursor = localcon.cursor(cursor_factory=DictCursor)
            cursor.execute(stats_query)
            stats[dbname] = cursor.fetchone() or {}
            cursor.close()
            localcon.close()
        return stats


class IndexDiskIO(IOPlugin):

    NAME = 'indexio'
    TITLE = 'Disk/Buffer IOs (Indices)'

    # Fields are processed as read/hit pairs. They need to follow this sequence
    # in this list!
    FIELDS = [
        ('idx_blks_read',
         'Index blocks',
         ''),
        ('idx_blks_hit',
         'Index blocks',
         'Number of index blocks read (-) / buffered (+)'),
    ]

    def get_stats(self):
        # we need to get stats for each DB. We need to create new connections
        # for each.
        dbnames = self.all_databases

        # Fetch stats for each DB
        stats_query = dedent(
            '''\
            SELECT
                SUM(idx_blks_read) AS idx_blks_read,
                SUM(idx_blks_hit) AS idx_blks_hit
            FROM pg_statio_user_indexes;
            ''')

        stats = {}
        for dbname in dbnames:
            try:
                localcon = connect(
                    construct_dsn(dbname,
                                  self.user,
                                  self.password,
                                  self.host,
                                  self.port))
            except Exception as exc:
                LOG.error(exc)
                continue

            cursor = localcon.cursor(cursor_factory=DictCursor)
            cursor.execute(stats_query)
            stats[dbname] = cursor.fetchone() or {}
            cursor.close()
            localcon.close()
        return stats


class SequencesDiskIO(IOPlugin):

    NAME = 'sequenceio'
    TITLE = 'Disk/Buffer IOs (Sequences)'

    # Fields are processed as read/hit pairs. They need to follow this sequence
    # in this list!
    FIELDS = [
        ('blks_read',
         'Sequence blocks',
         ''),
        ('blks_hit',
         'Sequence blocks',
         'Number of block IOs for sequences\\nread (-) / buffered (+)'),
    ]

    def get_stats(self):
        # we need to get stats for each DB. We need to create new connections
        # for each.
        dbnames = self.all_databases

        # Fetch stats for each DB
        stats_query = dedent(
            '''\
            SELECT
                SUM(idx_blks_read) AS idx_blks_read,
                SUM(idx_blks_hit) AS idx_blks_hit
            FROM pg_statio_user_indexes;
            ''')

        stats = {}
        for dbname in dbnames:
            try:
                localcon = connect(
                    construct_dsn(dbname,
                                  self.user,
                                  self.password,
                                  self.host,
                                  self.port))
            except Exception as exc:
                LOG.error(exc)
                continue

            cursor = localcon.cursor(cursor_factory=DictCursor)
            cursor.execute(stats_query)
            stats[dbname] = cursor.fetchone() or {}
            cursor.close()
            localcon.close()
        return stats


def main(action):
    dbname = getenv('PG_DBNAME', 'template1')
    user = getenv('PG_USER', 'postgres')
    password = getenv('PG_PASSWORD', '')
    host = getenv('PG_HOST', '')
    port = int(getenv('PG_PORT', 0))
    selected_plugins = getenv('PG_MULTIGRAPHS', '__all__')
    selected_plugins = {name.strip() for name in selected_plugins.split(',')}

    available_plugins = find_subclasses(PostgresPlugin)
    if '__all__' in selected_plugins:
        active_plugins = available_plugins
    else:
        active_plugins = [plugin for plugin in available_plugins
                          if plugin.NAME in selected_plugins]

    connection = connect(construct_dsn(
        dbname,
        user,
        password,
        host,
        port))

    try:
        for cls in active_plugins:
            instance = cls(connection, user, password, host, port)
            if action == 'config':
                instance.config()
            else:
                instance.fetch()
    finally:
        connection.close()


if __name__ == '__main__':
    logging.basicConfig()
    args = parse_args()
    main(args.action)
